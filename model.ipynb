{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cLO8aSIU2ExA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKidp3Kr3VY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "15edc7b8-6e69-4ad3-9373-2ec42b94b61a"
      },
      "cell_type": "code",
      "source": [
        "#read training data \n",
        "PATH = '/root/data/data_train.csv'\n",
        "\n",
        "raw_train_data = pd.read_csv(PATH)\n",
        "print(raw_train_data.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  num1  num2  num3  num4  num5  num6  num7  num8  num9   ...    cat6  \\\n",
            "0   0     2     5     0     1     0     0     0     0     0   ...     NaN   \n",
            "1   1     1     7     0     0     1     0     0     0     0   ...     NaN   \n",
            "2   2     5     9     0     0     1     0     0     0     0   ...     NaN   \n",
            "3   3     0     2     1     0     0     0     0     0     0   ...     0.0   \n",
            "4   4     0     0     1     0     0     0     0     0     0   ...     NaN   \n",
            "\n",
            "   cat7  cat8  cat9  cat10  cat11  cat12  cat13  cat14  target  \n",
            "0     0   1.0     4    1.0      0    0.0      1     12       0  \n",
            "1     0   NaN    11    1.0      1    2.0      1     19       0  \n",
            "2     0   NaN    14    1.0      1    2.0      1     60       0  \n",
            "3     0   1.0    11    1.0      1    3.0      1    104       0  \n",
            "4     0   NaN    14    1.0      1    2.0      1     82       0  \n",
            "\n",
            "[5 rows x 58 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xkV6AYPjlJT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "862d42e7-a60d-4e9c-f93d-84b02ad3d160"
      },
      "cell_type": "code",
      "source": [
        "#read test data\n",
        "PATH = '/root/data/data_test.csv'\n",
        "\n",
        "raw_test_data = pd.read_csv(PATH)\n",
        "print(raw_test_data.shape)\n",
        "num_test,_ = raw_test_data.shape\n",
        "#store test data id\n",
        "test_id = raw_test_data['id'].values\n",
        "print(raw_test_data.head(10))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(892816, 57)\n",
            "   id  num1  num2  num3  num4  num5  num6  num7  num8  num9  ...    cat5  \\\n",
            "0   0     0     8     0     1     0     0     0     0     0  ...     1.0   \n",
            "1   1     4     5     0     0     0     1     0     0     0  ...     1.0   \n",
            "2   2     5     3     0     0     0     1     0     0     0  ...     1.0   \n",
            "3   3     0     6     1     0     0     0     0     0     0  ...     1.0   \n",
            "4   4     5     7     0     0     0     1     0     0     0  ...     1.0   \n",
            "5   5     0     6     1     0     0     0     0     0     0  ...     1.0   \n",
            "6   6     0     3     0     1     0     0     0     0     0  ...     1.0   \n",
            "7   8     0     0     1     0     0     0     0     0     0  ...     1.0   \n",
            "8  10     0     7     0     1     0     0     0     0     0  ...     0.0   \n",
            "9  11     1     6     0     0     0     1     0     0     0  ...     0.0   \n",
            "\n",
            "   cat6  cat7  cat8  cat9  cat10  cat11  cat12  cat13  cat14  \n",
            "0   NaN     0   NaN     1    1.0      1    2.0      1     65  \n",
            "1   NaN     0   0.0    11    1.0      1    0.0      1    103  \n",
            "2   NaN     0   NaN    14    1.0      1    2.0      1     29  \n",
            "3   NaN     0   NaN     1    1.0      1    2.0      1     40  \n",
            "4   NaN     0   NaN    11    1.0      1    2.0      1    101  \n",
            "5   NaN     0   NaN    11    0.0      0    2.0      1     11  \n",
            "6   NaN     0   0.0     1    1.0      1    0.0      1     10  \n",
            "7   NaN     0   NaN    11    1.0      1    2.0      1    103  \n",
            "8   1.0     1   0.0     2    NaN      0    0.0      1    104  \n",
            "9   NaN     0   NaN     4    1.0      1    2.0      1    104  \n",
            "\n",
            "[10 rows x 57 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tb310u8l9Z5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1042
        },
        "outputId": "7b6a9ca6-63a6-4a71-e1b0-8ebf26d95260"
      },
      "cell_type": "code",
      "source": [
        "#counting total na values in each column\n",
        "total_na = raw_train_data.isna().sum()\n",
        "print(total_na)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id             0\n",
            "num1           0\n",
            "num2           0\n",
            "num3           0\n",
            "num4           0\n",
            "num5           0\n",
            "num6           0\n",
            "num7           0\n",
            "num8           0\n",
            "num9           0\n",
            "num10          0\n",
            "num11          0\n",
            "num12          0\n",
            "num13          0\n",
            "num14          0\n",
            "num15          0\n",
            "num16          0\n",
            "num17          0\n",
            "num18     107909\n",
            "num19          5\n",
            "num20          1\n",
            "num21          0\n",
            "num22      42667\n",
            "num23          0\n",
            "der1           0\n",
            "der2           0\n",
            "der3           0\n",
            "der4           0\n",
            "der5           0\n",
            "der6           0\n",
            "der7           0\n",
            "der8           0\n",
            "der9           0\n",
            "der10          0\n",
            "der11          0\n",
            "der12          0\n",
            "der13          0\n",
            "der14          0\n",
            "der15          0\n",
            "der16          0\n",
            "der17          0\n",
            "der18          0\n",
            "der19          0\n",
            "cat1         217\n",
            "cat2          83\n",
            "cat3        5814\n",
            "cat4         107\n",
            "cat5           5\n",
            "cat6      411792\n",
            "cat7           0\n",
            "cat8      266928\n",
            "cat9           0\n",
            "cat10      11503\n",
            "cat11          0\n",
            "cat12        570\n",
            "cat13          0\n",
            "cat14          0\n",
            "target         0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JDWisjDTzLeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "fa35c792-f45d-44d7-8437-d903b40defd7"
      },
      "cell_type": "code",
      "source": [
        "sns.countplot(\"target\",data=raw_train_data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1428: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
            "  stat_data = remove_na(group_data)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8358c942e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEJCAYAAABR4cpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhokkqZlhIklqZphIkpoZJpKkZv8fahEkT2ipQHwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8355fbb390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gqnOQbGDw3T7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4fcf163-5004-4569-a645-74eb888e1d55"
      },
      "cell_type": "code",
      "source": [
        "#As number of labels belonging to class 1 is very less so we will use oversampling\n",
        "index = list(raw_train_data[raw_train_data['target']==1].index)\n",
        "class1_data = raw_train_data.iloc[index]\n",
        "lst = [class1_data] * 10\n",
        "class1_data = pd.concat(lst)\n",
        "lst = [raw_train_data,class1_data]\n",
        "raw_train_data = pd.concat(lst)\n",
        "num_training,_=raw_train_data.shape\n",
        "print(raw_train_data.shape)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(813160, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JMDL8zUsSr48",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**\n",
        "\n",
        "\n",
        "\n",
        "1.   Replace missing values in numerical variables by mean\n",
        "2.   Replace missing values in categorical variables by creating new category.\n",
        "\n",
        "1.  Convert all numerical values between 0 and  1\n",
        "\n",
        "1.   Convert all categorical values into label encoding and one hot encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OkKnIiXpRM8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def one_hot_encoding(data,column_names):\n",
        "  \n",
        "  data = pd.get_dummies(data , columns = column_names , prefix = column_names)\n",
        "  return data  \n",
        "\n",
        "\n",
        "def normalize_values(data,column_names):\n",
        "  num_min = data[column_names].min()\n",
        "  num_max = data[column_names].max()\n",
        "  \n",
        "  norm_val = num_max - num_min\n",
        "  data[column_names] = (data[column_names] - num_min) / norm_val\n",
        "  \n",
        "  return data\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "  \n",
        "  column_names = list(data.columns.values)\n",
        "  num_columns = column_names[1:24]\n",
        "  #der_columns = column_names[24:43]\n",
        "  der_numerical_columns = column_names[24:27]\n",
        "  der_categorical_columns = column_names[27:43]\n",
        "  cat_columns = column_names[43:]\n",
        "  \n",
        "  #handling missing values in numerical variables \n",
        "  data[num_columns] = data[num_columns].fillna(data[num_columns].mean())\n",
        "  \n",
        "  \n",
        "  #drop some columns \n",
        "  drop_columns = ['cat6','cat8']\n",
        "  \n",
        "  cat_columns.remove('cat6')\n",
        "  cat_columns.remove('cat8')\n",
        "  \n",
        "  data = data.drop(columns = drop_columns)\n",
        "  \n",
        "  #handling missing values in categorical variables \n",
        "  data[cat_columns] = data[cat_columns].fillna(\"NA\")\n",
        "  \n",
        "  #label encoding \n",
        "  cat_datatype_columns = data.select_dtypes(['object']).columns\n",
        "  \n",
        "  for column in cat_datatype_columns:\n",
        "    data[column] = data[column].astype('category')\n",
        "    \n",
        "  data[cat_datatype_columns] = data[cat_datatype_columns].apply(lambda x:x.cat.codes)\n",
        "  \n",
        "  #one hot encoding\n",
        "  data = one_hot_encoding(data,der_categorical_columns)\n",
        "  data = one_hot_encoding(data,cat_columns)\n",
        "  \n",
        "  \n",
        "  #normalize numerical values\n",
        "  data = normalize_values(data,num_columns)\n",
        "  #data = normalize_values(data,der_columns)\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ui38akUtbX-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8b4078f-bdf1-406f-bca0-f1a97994aae3"
      },
      "cell_type": "code",
      "source": [
        "#seperate out the target values from the training data\n",
        "target = raw_train_data['target'].values\n",
        "raw_train_data = raw_train_data.drop(columns = ['target'])\n",
        "num_training,_ = raw_train_data.shape\n",
        "print(raw_train_data.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(813160, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EiH_Ppkpnr9Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d90d8d53-9163-447c-926d-8f543d2883be"
      },
      "cell_type": "code",
      "source": [
        "#combining train and test data to preprocess\n",
        "data = [raw_train_data , raw_test_data]\n",
        "data = pd.concat(data)\n",
        "print(data.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1705976, 57)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8FoICUzoAO4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "prep_data = preprocess_data(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FHC8nzBeoO5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4d50254a-c299-4510-c8a4-aca2b02adf13"
      },
      "cell_type": "code",
      "source": [
        "print(prep_data.shape)\n",
        "#dividing into train and test\n",
        "prep_train_data = prep_data[:num_training]\n",
        "prep_test_data = prep_data[num_training:num_training + num_test]\n",
        "print(prep_train_data.shape)\n",
        "print(prep_test_data.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1705976, 369)\n",
            "(813160, 369)\n",
            "(892816, 369)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EVpo6dV5OnKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "dcbede7a-5d67-4f8e-8c76-70767d616eee"
      },
      "cell_type": "code",
      "source": [
        "print(prep_train_data.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id      num1      num2  num3  num4  num5  num6  num7  num8  num9  \\\n",
            "0   0  0.285714  0.454545   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
            "1   1  0.142857  0.636364   0.0   0.0   1.0   0.0   0.0   0.0   0.0   \n",
            "2   2  0.714286  0.818182   0.0   0.0   1.0   0.0   0.0   0.0   0.0   \n",
            "3   3  0.000000  0.181818   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
            "4   4  0.000000  0.000000   1.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
            "\n",
            "     ...      cat14_95  cat14_96  cat14_97  cat14_98  cat14_99  cat14_100  \\\n",
            "0    ...             0         0         0         0         0          0   \n",
            "1    ...             0         0         0         0         0          0   \n",
            "2    ...             0         0         0         0         0          0   \n",
            "3    ...             0         0         0         0         0          0   \n",
            "4    ...             0         0         0         0         0          0   \n",
            "\n",
            "   cat14_101  cat14_102  cat14_103  cat14_104  \n",
            "0          0          0          0          0  \n",
            "1          0          0          0          0  \n",
            "2          0          0          0          0  \n",
            "3          0          0          0          1  \n",
            "4          0          0          0          0  \n",
            "\n",
            "[5 rows x 369 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "51eb3-O3T5Hg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define training and validation size\n",
        "\n",
        "n_training = 717160\n",
        "n_validation = 96000\n",
        "n_test = 892816\n",
        "\n",
        "\n",
        "    \n",
        "#Divide data into training set and validation set\n",
        "\n",
        "# removing id column\n",
        "def get_train_test_data(prep_train_data,target,prep_test_data):\n",
        "  prep_train_data = prep_train_data.drop(columns = ['id'])\n",
        "  prep_test_data = prep_test_data.drop(columns = ['id'])\n",
        "  y_train = target\n",
        "  x_train = prep_train_data.values\n",
        "  N,M = x_train.shape\n",
        "  indices = np.arange(N)\n",
        "  np.random.shuffle(indices)\n",
        "  x_train = x_train[indices]\n",
        "  y_train = y_train[indices]\n",
        "\n",
        "\n",
        "  x_val = x_train[:n_validation]\n",
        "  y_val = y_train[:n_validation]\n",
        "  val_set = (x_val,y_val)\n",
        "\n",
        "\n",
        "  x_train = x_train[n_validation:n_validation+n_training]\n",
        "  y_train = y_train[n_validation:n_validation+n_training]\n",
        "  train_set = (x_train,y_train)\n",
        "  \n",
        "  print(x_train.shape)\n",
        "  print(y_train.shape)\n",
        "  print(x_val.shape)\n",
        "  print(y_val.shape)\n",
        "  \n",
        "  #including a random y_test so iterator is compatible\n",
        "  y_test = np.zeros((100000),dtype=int)\n",
        "  x_test = prep_test_data.values\n",
        "  x_test = x_test[:100000]\n",
        "  return train_set,val_set,(x_test,y_test)\n",
        "  \n",
        "\n",
        "class Model():\n",
        "  def __init__(self):\n",
        "    self.lrate = 0.005\n",
        "    self.batch_size = 64 \n",
        "    self.ntrain = 717160\n",
        "    self.nclasses = 2\n",
        "    self.ntest = 892816\n",
        "    self.training =True\n",
        "    self.keep_prob = tf.constant(0.90)\n",
        "    self.hidden_size = 128\n",
        "    \n",
        "  def get_data(self,prep_train_data,target,prep_test_data):\n",
        "    with tf.name_scope('data'):\n",
        "      # Create dataset and iterator\n",
        "      train,val,test = get_train_test_data(prep_train_data,target,prep_test_data)\n",
        "      \n",
        "      train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "      train_data = train_data.shuffle(100000)\n",
        "      train_data = train_data.batch(self.batch_size)\n",
        "\n",
        "      val_data = tf.data.Dataset.from_tensor_slices(val)\n",
        "      val_data = val_data.batch(self.batch_size)\n",
        "\n",
        "\n",
        "      test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "      test_data = test_data.batch(self.batch_size)\n",
        "\n",
        "      iterator = tf.data.Iterator.from_structure(train_data.output_types,\n",
        "                                                train_data.output_shapes)\n",
        "      \n",
        "      X , label = iterator.get_next()\n",
        "      _,self.m = X.shape\n",
        "      self.X = tf.cast(X,dtype=tf.float32)\n",
        "      self.label = tf.one_hot(label,self.nclasses)\n",
        "\n",
        "      self.train_init = iterator.make_initializer(train_data)\n",
        "      self.val_init = iterator.make_initializer(val_data)\n",
        "      self.test_init = iterator.make_initializer(test_data)\n",
        "      \n",
        "\n",
        "  def inference(self):\n",
        "    with tf.variable_scope('logreg',reuse=tf.AUTO_REUSE) as scope:\n",
        "      # Create weights and bias\n",
        "      # w is initialized to random variables with mean 0 and stddev 0.01 \n",
        "      # b is initialized to zero\n",
        "      w = tf.get_variable(name='weights',dtype=tf.float32,shape=[32,self.nclasses],\n",
        "                          initializer = tf.random_normal_initializer(0 , 0.01))\n",
        "      b = tf.get_variable(name='bias',shape=[self.nclasses],\n",
        "                         initializer = tf.zeros_initializer())\n",
        "\n",
        "      # build model \n",
        "      # the model that returns logits\n",
        "      layer1 = tf.layers.dense(self.X,128,activation=tf.nn.relu,\n",
        "                               kernel_initializer=tf.random_normal_initializer(0,0.01),\n",
        "                               name='dense_layer1')\n",
        "      #dropout1 = tf.nn.dropout(layer1,self.keep_prob,name='dropout1')\n",
        "      \n",
        "      layer2 = tf.layers.dense(layer1,64,activation=tf.nn.relu,\n",
        "                               kernel_initializer = tf.random_normal_initializer(0,0.01),\n",
        "                               name='dense_layer2')\n",
        "      #dropout2 = tf.nn.dropout(layer2,self.keep_prob,name='dropout2')\n",
        "      \n",
        "      layer3 = tf.layers.dense(layer2,64,activation=tf.nn.relu,\n",
        "                               kernel_initializer = tf.random_normal_initializer(0,0.01),\n",
        "                               name='dense_layer3')\n",
        "      \n",
        "      layer4 = tf.layers.dense(layer3,32,activation=tf.nn.relu,\n",
        "                               kernel_initializer = tf.random_normal_initializer(0,0.01),\n",
        "                               name='dense_layer4')\n",
        "      \n",
        "      self.logits = tf.matmul(layer4,w) + b\n",
        "\n",
        "  def create_loss(self):\n",
        "    with tf.name_scope('loss'):\n",
        "      # define loss function\n",
        "      entropy = tf.nn.softmax_cross_entropy_with_logits(logits = self.logits , labels = self.label)\n",
        "      self.loss = tf.reduce_mean(entropy,name='loss')\n",
        "      \n",
        "  def create_optimizer(self):\n",
        "    with tf.name_scope('optimizer'):      \n",
        "      # define training op \n",
        "      self.optimizer = tf.train.AdamOptimizer(self.lrate).minimize(self.loss)\n",
        "\n",
        "  def eval_model(self):\n",
        "    with tf.name_scope('eval'):      \n",
        "      # calculate accuracy \n",
        "      self.preds = tf.nn.softmax(self.logits)\n",
        "      self.predicted_labels = tf.argmax(self.preds,1)\n",
        "      correct_preds = tf.equal(tf.argmax(self.preds,1),tf.argmax(self.label,1))\n",
        "      self.accuracy = tf.reduce_sum(tf.cast(correct_preds,dtype=tf.float32))\n",
        "\n",
        "  def build_model(self,prep_train_data,target,prep_test_data):\n",
        "    self.get_data(prep_train_data,target,prep_test_data)\n",
        "    self.inference()\n",
        "    self.create_loss()\n",
        "    self.create_optimizer()\n",
        "    self.eval_model()\n",
        "    \n",
        "  def train(self,n_epochs):\n",
        "    # start training loop \n",
        "    init = tf.global_variables_initializer()\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for epoch in range(n_epochs):\n",
        "        sess.run(self.train_init)\n",
        "        total_loss = 0.0\n",
        "        n_batches = 0\n",
        "        step = 0\n",
        "        train_acc = 0\n",
        "        \n",
        "        try:\n",
        "          while True:\n",
        "            _,batch_loss,acc = sess.run([self.optimizer,self.loss,self.accuracy])\n",
        "            total_loss += batch_loss\n",
        "            train_acc += acc\n",
        "            n_batches += 1\n",
        "            step += 1\n",
        "            if step%100 == 0:\n",
        "              print('Step {} : Loss {}'.format(step,batch_loss))\n",
        "        \n",
        "        except tf.errors.OutOfRangeError:\n",
        "          pass\n",
        "        print('Average loss at epoch {} is {}'.format(epoch,total_loss/n_batches))\n",
        "        print('Average train accuracy at epoch {} is {}'.format(epoch,train_acc/self.ntrain))\n",
        "      \n",
        "      #calculate validation set accuracy\n",
        "      sess.run(self.val_init)\n",
        "      total_acc = 0\n",
        "      val_pred = []\n",
        "      try:\n",
        "        while True:\n",
        "          acc,pred = sess.run([self.accuracy,self.predicted_labels])\n",
        "          total_acc += acc\n",
        "          val_pred.append(pred)\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "      print('Average Validation accuracy {}'.format(total_acc / 96000))\n",
        "      \n",
        "      #calculate test set predictions\n",
        "      test_pred = []\n",
        "      \n",
        "      sess.run(self.test_init)\n",
        "      self.training = False\n",
        "      try:\n",
        "        while True:\n",
        "          pred = sess.run(self.predicted_labels)\n",
        "          test_pred.append(pred)\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    return val_pred,test_pred\n",
        "       \n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmFmxHjJSYFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e60e8b8a-f104-4876-dac6-c291e9ba7ebc"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Build Model\n",
        "model = Model()\n",
        "model.build_model(prep_train_data,target,prep_test_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(717160, 368)\n",
            "(717160,)\n",
            "(96000, 368)\n",
            "(96000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SEY7NKZsezoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20180
        },
        "outputId": "4b1e72cb-ca68-48c8-a11e-9ce7c7c24bce"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.train(20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 100 : Loss 0.6424429416656494\n",
            "Step 200 : Loss 0.6326107978820801\n",
            "Step 300 : Loss 0.6020940542221069\n",
            "Step 400 : Loss 0.569532036781311\n",
            "Step 500 : Loss 0.5899226665496826\n",
            "Step 600 : Loss 0.6144356727600098\n",
            "Step 700 : Loss 0.6213679313659668\n",
            "Step 800 : Loss 0.5956388711929321\n",
            "Step 900 : Loss 0.590300440788269\n",
            "Step 1000 : Loss 0.6186176538467407\n",
            "Step 1100 : Loss 0.5893237590789795\n",
            "Step 1200 : Loss 0.5631606578826904\n",
            "Step 1300 : Loss 0.570693850517273\n",
            "Step 1400 : Loss 0.615501880645752\n",
            "Step 1500 : Loss 0.580223560333252\n",
            "Step 1600 : Loss 0.6161344051361084\n",
            "Step 1700 : Loss 0.5393094420433044\n",
            "Step 1800 : Loss 0.6201529502868652\n",
            "Step 1900 : Loss 0.6208126544952393\n",
            "Step 2000 : Loss 0.6059439182281494\n",
            "Step 2100 : Loss 0.6077674031257629\n",
            "Step 2200 : Loss 0.5789238810539246\n",
            "Step 2300 : Loss 0.5658040046691895\n",
            "Step 2400 : Loss 0.5295243263244629\n",
            "Step 2500 : Loss 0.5890027284622192\n",
            "Step 2600 : Loss 0.5827115178108215\n",
            "Step 2700 : Loss 0.6104682683944702\n",
            "Step 2800 : Loss 0.5885738730430603\n",
            "Step 2900 : Loss 0.5797473192214966\n",
            "Step 3000 : Loss 0.560106635093689\n",
            "Step 3100 : Loss 0.5811197757720947\n",
            "Step 3200 : Loss 0.59553062915802\n",
            "Step 3300 : Loss 0.6114985942840576\n",
            "Step 3400 : Loss 0.6495668888092041\n",
            "Step 3500 : Loss 0.5322617292404175\n",
            "Step 3600 : Loss 0.5884079933166504\n",
            "Step 3700 : Loss 0.6007568836212158\n",
            "Step 3800 : Loss 0.5880844593048096\n",
            "Step 3900 : Loss 0.6387354135513306\n",
            "Step 4000 : Loss 0.5175722241401672\n",
            "Step 4100 : Loss 0.5606050491333008\n",
            "Step 4200 : Loss 0.5619046688079834\n",
            "Step 4300 : Loss 0.6249897480010986\n",
            "Step 4400 : Loss 0.5584784746170044\n",
            "Step 4500 : Loss 0.5894147753715515\n",
            "Step 4600 : Loss 0.5488674640655518\n",
            "Step 4700 : Loss 0.5800756216049194\n",
            "Step 4800 : Loss 0.6047986149787903\n",
            "Step 4900 : Loss 0.600502610206604\n",
            "Step 5000 : Loss 0.5567495822906494\n",
            "Step 5100 : Loss 0.5950731635093689\n",
            "Step 5200 : Loss 0.5619131326675415\n",
            "Step 5300 : Loss 0.5311098098754883\n",
            "Step 5400 : Loss 0.5822449326515198\n",
            "Step 5500 : Loss 0.559600293636322\n",
            "Step 5600 : Loss 0.5827214121818542\n",
            "Average loss at epoch 0 is 0.5871620892989046\n",
            "Average train accuracy at epoch 0 is 0.7090607395839144\n",
            "Step 100 : Loss 0.5719732642173767\n",
            "Step 200 : Loss 0.5700724124908447\n",
            "Step 300 : Loss 0.5629907250404358\n",
            "Step 400 : Loss 0.5828763842582703\n",
            "Step 500 : Loss 0.6318058967590332\n",
            "Step 600 : Loss 0.5747582316398621\n",
            "Step 700 : Loss 0.6090487241744995\n",
            "Step 800 : Loss 0.6074179410934448\n",
            "Step 900 : Loss 0.6093518733978271\n",
            "Step 1000 : Loss 0.5535712242126465\n",
            "Step 1100 : Loss 0.614620566368103\n",
            "Step 1200 : Loss 0.5651415586471558\n",
            "Step 1300 : Loss 0.5618846416473389\n",
            "Step 1400 : Loss 0.5500069856643677\n",
            "Step 1500 : Loss 0.5764328837394714\n",
            "Step 1600 : Loss 0.5949947834014893\n",
            "Step 1700 : Loss 0.49708443880081177\n",
            "Step 1800 : Loss 0.5727486610412598\n",
            "Step 1900 : Loss 0.5413376092910767\n",
            "Step 2000 : Loss 0.5679489374160767\n",
            "Step 2100 : Loss 0.6056780815124512\n",
            "Step 2200 : Loss 0.6243146657943726\n",
            "Step 2300 : Loss 0.5305414795875549\n",
            "Step 2400 : Loss 0.6071691513061523\n",
            "Step 2500 : Loss 0.5855302810668945\n",
            "Step 2600 : Loss 0.5940055847167969\n",
            "Step 2700 : Loss 0.5393280982971191\n",
            "Step 2800 : Loss 0.5555658936500549\n",
            "Step 2900 : Loss 0.6572631597518921\n",
            "Step 3000 : Loss 0.551039457321167\n",
            "Step 3100 : Loss 0.5851706862449646\n",
            "Step 3200 : Loss 0.6246515512466431\n",
            "Step 3300 : Loss 0.5483647584915161\n",
            "Step 3400 : Loss 0.5716246366500854\n",
            "Step 3500 : Loss 0.6084972620010376\n",
            "Step 3600 : Loss 0.5839590430259705\n",
            "Step 3700 : Loss 0.6442859172821045\n",
            "Step 3800 : Loss 0.5827807188034058\n",
            "Step 3900 : Loss 0.5335359573364258\n",
            "Step 4000 : Loss 0.5994535684585571\n",
            "Step 4100 : Loss 0.5685360431671143\n",
            "Step 4200 : Loss 0.5417159795761108\n",
            "Step 4300 : Loss 0.5785911083221436\n",
            "Step 4400 : Loss 0.490224689245224\n",
            "Step 4500 : Loss 0.6380629539489746\n",
            "Step 4600 : Loss 0.5612805485725403\n",
            "Step 4700 : Loss 0.6351420879364014\n",
            "Step 4800 : Loss 0.6301742792129517\n",
            "Step 4900 : Loss 0.5825704336166382\n",
            "Step 5000 : Loss 0.5783446431159973\n",
            "Step 5100 : Loss 0.6282704472541809\n",
            "Step 5200 : Loss 0.5422362089157104\n",
            "Step 5300 : Loss 0.5955758094787598\n",
            "Step 5400 : Loss 0.5855467319488525\n",
            "Step 5500 : Loss 0.5690302848815918\n",
            "Step 5600 : Loss 0.5414950847625732\n",
            "Average loss at epoch 1 is 0.580476416262605\n",
            "Average train accuracy at epoch 1 is 0.7115525126889397\n",
            "Step 100 : Loss 0.602768063545227\n",
            "Step 200 : Loss 0.6078413724899292\n",
            "Step 300 : Loss 0.5659167766571045\n",
            "Step 400 : Loss 0.6407030820846558\n",
            "Step 500 : Loss 0.5297677516937256\n",
            "Step 600 : Loss 0.5623761415481567\n",
            "Step 700 : Loss 0.5255377292633057\n",
            "Step 800 : Loss 0.5436912775039673\n",
            "Step 900 : Loss 0.5864380598068237\n",
            "Step 1000 : Loss 0.5673622488975525\n",
            "Step 1100 : Loss 0.5557021498680115\n",
            "Step 1200 : Loss 0.5748502016067505\n",
            "Step 1300 : Loss 0.5988650321960449\n",
            "Step 1400 : Loss 0.5411444902420044\n",
            "Step 1500 : Loss 0.5257714986801147\n",
            "Step 1600 : Loss 0.5826320052146912\n",
            "Step 1700 : Loss 0.5699667930603027\n",
            "Step 1800 : Loss 0.6287192106246948\n",
            "Step 1900 : Loss 0.6127974987030029\n",
            "Step 2000 : Loss 0.5739055871963501\n",
            "Step 2100 : Loss 0.6318972110748291\n",
            "Step 2200 : Loss 0.5818007588386536\n",
            "Step 2300 : Loss 0.5333764553070068\n",
            "Step 2400 : Loss 0.5283284187316895\n",
            "Step 2500 : Loss 0.5061139464378357\n",
            "Step 2600 : Loss 0.5657081604003906\n",
            "Step 2700 : Loss 0.5748928189277649\n",
            "Step 2800 : Loss 0.6120027899742126\n",
            "Step 2900 : Loss 0.5690552592277527\n",
            "Step 3000 : Loss 0.5627682209014893\n",
            "Step 3100 : Loss 0.5144752264022827\n",
            "Step 3200 : Loss 0.5340458750724792\n",
            "Step 3300 : Loss 0.5960485339164734\n",
            "Step 3400 : Loss 0.6071614623069763\n",
            "Step 3500 : Loss 0.520456075668335\n",
            "Step 3600 : Loss 0.5032983422279358\n",
            "Step 3700 : Loss 0.6507806181907654\n",
            "Step 3800 : Loss 0.566632866859436\n",
            "Step 3900 : Loss 0.5411279797554016\n",
            "Step 4000 : Loss 0.609542965888977\n",
            "Step 4100 : Loss 0.5832865238189697\n",
            "Step 4200 : Loss 0.6281914710998535\n",
            "Step 4300 : Loss 0.58060222864151\n",
            "Step 4400 : Loss 0.5733543038368225\n",
            "Step 4500 : Loss 0.6199158430099487\n",
            "Step 4600 : Loss 0.5701302289962769\n",
            "Step 4700 : Loss 0.5883663296699524\n",
            "Step 4800 : Loss 0.5719177722930908\n",
            "Step 4900 : Loss 0.5687249898910522\n",
            "Step 5000 : Loss 0.5919744968414307\n",
            "Step 5100 : Loss 0.6158117055892944\n",
            "Step 5200 : Loss 0.5855636596679688\n",
            "Step 5300 : Loss 0.5838515162467957\n",
            "Step 5400 : Loss 0.5640749931335449\n",
            "Step 5500 : Loss 0.5324827432632446\n",
            "Step 5600 : Loss 0.6139596700668335\n",
            "Average loss at epoch 2 is 0.576907903753858\n",
            "Average train accuracy at epoch 2 is 0.7138699871716214\n",
            "Step 100 : Loss 0.5609541535377502\n",
            "Step 200 : Loss 0.5227891206741333\n",
            "Step 300 : Loss 0.6374435424804688\n",
            "Step 400 : Loss 0.5238328576087952\n",
            "Step 500 : Loss 0.5555203557014465\n",
            "Step 600 : Loss 0.5033905506134033\n",
            "Step 700 : Loss 0.5461519956588745\n",
            "Step 800 : Loss 0.5768414735794067\n",
            "Step 900 : Loss 0.5774485468864441\n",
            "Step 1000 : Loss 0.5531331300735474\n",
            "Step 1100 : Loss 0.5218497514724731\n",
            "Step 1200 : Loss 0.5415241718292236\n",
            "Step 1300 : Loss 0.5588086247444153\n",
            "Step 1400 : Loss 0.5079753398895264\n",
            "Step 1500 : Loss 0.6070512533187866\n",
            "Step 1600 : Loss 0.5456651449203491\n",
            "Step 1700 : Loss 0.5704429149627686\n",
            "Step 1800 : Loss 0.5468482971191406\n",
            "Step 1900 : Loss 0.5515265464782715\n",
            "Step 2000 : Loss 0.6470679640769958\n",
            "Step 2100 : Loss 0.5404738187789917\n",
            "Step 2200 : Loss 0.6115490198135376\n",
            "Step 2300 : Loss 0.5631606578826904\n",
            "Step 2400 : Loss 0.5865374207496643\n",
            "Step 2500 : Loss 0.5232840776443481\n",
            "Step 2600 : Loss 0.6046324968338013\n",
            "Step 2700 : Loss 0.5388087034225464\n",
            "Step 2800 : Loss 0.624760627746582\n",
            "Step 2900 : Loss 0.5530992150306702\n",
            "Step 3000 : Loss 0.5856472253799438\n",
            "Step 3100 : Loss 0.5538322925567627\n",
            "Step 3200 : Loss 0.612265408039093\n",
            "Step 3300 : Loss 0.5577551126480103\n",
            "Step 3400 : Loss 0.6322105526924133\n",
            "Step 3500 : Loss 0.6053651571273804\n",
            "Step 3600 : Loss 0.5283835530281067\n",
            "Step 3700 : Loss 0.5283021926879883\n",
            "Step 3800 : Loss 0.5495172142982483\n",
            "Step 3900 : Loss 0.5755330920219421\n",
            "Step 4000 : Loss 0.5477433204650879\n",
            "Step 4100 : Loss 0.5967961549758911\n",
            "Step 4200 : Loss 0.5245758891105652\n",
            "Step 4300 : Loss 0.5703878402709961\n",
            "Step 4400 : Loss 0.5733418464660645\n",
            "Step 4500 : Loss 0.5362969636917114\n",
            "Step 4600 : Loss 0.5937864780426025\n",
            "Step 4700 : Loss 0.561486005783081\n",
            "Step 4800 : Loss 0.5051052570343018\n",
            "Step 4900 : Loss 0.5318050384521484\n",
            "Step 5000 : Loss 0.5478370189666748\n",
            "Step 5100 : Loss 0.5495535731315613\n",
            "Step 5200 : Loss 0.5280234217643738\n",
            "Step 5300 : Loss 0.5279070138931274\n",
            "Step 5400 : Loss 0.5831231474876404\n",
            "Step 5500 : Loss 0.633008599281311\n",
            "Step 5600 : Loss 0.5962719917297363\n",
            "Average loss at epoch 3 is 0.5674182092337103\n",
            "Average train accuracy at epoch 3 is 0.7207959172290702\n",
            "Step 100 : Loss 0.5041643381118774\n",
            "Step 200 : Loss 0.5469907522201538\n",
            "Step 300 : Loss 0.5330430865287781\n",
            "Step 400 : Loss 0.4650883674621582\n",
            "Step 500 : Loss 0.5504888892173767\n",
            "Step 600 : Loss 0.6326467990875244\n",
            "Step 700 : Loss 0.5128718614578247\n",
            "Step 800 : Loss 0.5785900950431824\n",
            "Step 900 : Loss 0.5277022123336792\n",
            "Step 1000 : Loss 0.483448326587677\n",
            "Step 1100 : Loss 0.5052467584609985\n",
            "Step 1200 : Loss 0.626564085483551\n",
            "Step 1300 : Loss 0.5286621451377869\n",
            "Step 1400 : Loss 0.5349747538566589\n",
            "Step 1500 : Loss 0.625220537185669\n",
            "Step 1600 : Loss 0.626913845539093\n",
            "Step 1700 : Loss 0.634069561958313\n",
            "Step 1800 : Loss 0.5741273164749146\n",
            "Step 1900 : Loss 0.5712987184524536\n",
            "Step 2000 : Loss 0.48618361353874207\n",
            "Step 2100 : Loss 0.5182325839996338\n",
            "Step 2200 : Loss 0.48017382621765137\n",
            "Step 2300 : Loss 0.5856244564056396\n",
            "Step 2400 : Loss 0.5431610345840454\n",
            "Step 2500 : Loss 0.6150875687599182\n",
            "Step 2600 : Loss 0.6197631359100342\n",
            "Step 2700 : Loss 0.5773578882217407\n",
            "Step 2800 : Loss 0.541836142539978\n",
            "Step 2900 : Loss 0.5192312598228455\n",
            "Step 3000 : Loss 0.5304434299468994\n",
            "Step 3100 : Loss 0.5778367519378662\n",
            "Step 3200 : Loss 0.5107043981552124\n",
            "Step 3300 : Loss 0.6204580664634705\n",
            "Step 3400 : Loss 0.624072253704071\n",
            "Step 3500 : Loss 0.510075032711029\n",
            "Step 3600 : Loss 0.5324629545211792\n",
            "Step 3700 : Loss 0.5427100658416748\n",
            "Step 3800 : Loss 0.47260719537734985\n",
            "Step 3900 : Loss 0.5815274715423584\n",
            "Step 4000 : Loss 0.45703062415122986\n",
            "Step 4100 : Loss 0.49137404561042786\n",
            "Step 4200 : Loss 0.6106617450714111\n",
            "Step 4300 : Loss 0.5404808521270752\n",
            "Step 4400 : Loss 0.5758480429649353\n",
            "Step 4500 : Loss 0.5068174600601196\n",
            "Step 4600 : Loss 0.5772935152053833\n",
            "Step 4700 : Loss 0.6328586339950562\n",
            "Step 4800 : Loss 0.5577957034111023\n",
            "Step 4900 : Loss 0.5798166394233704\n",
            "Step 5000 : Loss 0.611944854259491\n",
            "Step 5100 : Loss 0.6310885548591614\n",
            "Step 5200 : Loss 0.4928424656391144\n",
            "Step 5300 : Loss 0.5330370664596558\n",
            "Step 5400 : Loss 0.5631639361381531\n",
            "Step 5500 : Loss 0.5797159671783447\n",
            "Step 5600 : Loss 0.5277276039123535\n",
            "Average loss at epoch 4 is 0.5511108925837014\n",
            "Average train accuracy at epoch 4 is 0.7339240894639969\n",
            "Step 100 : Loss 0.5690088272094727\n",
            "Step 200 : Loss 0.5367250442504883\n",
            "Step 300 : Loss 0.4944795072078705\n",
            "Step 400 : Loss 0.6013440489768982\n",
            "Step 500 : Loss 0.5720003843307495\n",
            "Step 600 : Loss 0.558912992477417\n",
            "Step 700 : Loss 0.5089958906173706\n",
            "Step 800 : Loss 0.476074755191803\n",
            "Step 900 : Loss 0.5504295825958252\n",
            "Step 1000 : Loss 0.5361558794975281\n",
            "Step 1100 : Loss 0.6287516355514526\n",
            "Step 1200 : Loss 0.5366237163543701\n",
            "Step 1300 : Loss 0.6225463151931763\n",
            "Step 1400 : Loss 0.5935229063034058\n",
            "Step 1500 : Loss 0.6019272804260254\n",
            "Step 1600 : Loss 0.6464970707893372\n",
            "Step 1700 : Loss 0.6200600266456604\n",
            "Step 1800 : Loss 0.5309815406799316\n",
            "Step 1900 : Loss 0.5299892425537109\n",
            "Step 2000 : Loss 0.5234572887420654\n",
            "Step 2100 : Loss 0.5956743955612183\n",
            "Step 2200 : Loss 0.5711554288864136\n",
            "Step 2300 : Loss 0.5398456454277039\n",
            "Step 2400 : Loss 0.5877508521080017\n",
            "Step 2500 : Loss 0.4951949119567871\n",
            "Step 2600 : Loss 0.6177766919136047\n",
            "Step 2700 : Loss 0.5534415245056152\n",
            "Step 2800 : Loss 0.5549381375312805\n",
            "Step 2900 : Loss 0.4795941710472107\n",
            "Step 3000 : Loss 0.5598140954971313\n",
            "Step 3100 : Loss 0.4925428032875061\n",
            "Step 3200 : Loss 0.588585376739502\n",
            "Step 3300 : Loss 0.552700400352478\n",
            "Step 3400 : Loss 0.4799264371395111\n",
            "Step 3500 : Loss 0.5190137028694153\n",
            "Step 3600 : Loss 0.5387517213821411\n",
            "Step 3700 : Loss 0.5184735655784607\n",
            "Step 3800 : Loss 0.5189211368560791\n",
            "Step 3900 : Loss 0.5755090713500977\n",
            "Step 4000 : Loss 0.5529685020446777\n",
            "Step 4100 : Loss 0.46592605113983154\n",
            "Step 4200 : Loss 0.5188626646995544\n",
            "Step 4300 : Loss 0.530529260635376\n",
            "Step 4400 : Loss 0.5328493118286133\n",
            "Step 4500 : Loss 0.5159608125686646\n",
            "Step 4600 : Loss 0.5423108339309692\n",
            "Step 4700 : Loss 0.4635469317436218\n",
            "Step 4800 : Loss 0.5787106156349182\n",
            "Step 4900 : Loss 0.5038948655128479\n",
            "Step 5000 : Loss 0.5041778087615967\n",
            "Step 5100 : Loss 0.5497364401817322\n",
            "Step 5200 : Loss 0.568619966506958\n",
            "Step 5300 : Loss 0.5424666404724121\n",
            "Step 5400 : Loss 0.5992657542228699\n",
            "Step 5500 : Loss 0.48230960965156555\n",
            "Step 5600 : Loss 0.49878227710723877\n",
            "Average loss at epoch 5 is 0.5360116597564863\n",
            "Average train accuracy at epoch 5 is 0.7453162474203804\n",
            "Step 100 : Loss 0.510015606880188\n",
            "Step 200 : Loss 0.5298528671264648\n",
            "Step 300 : Loss 0.5389277338981628\n",
            "Step 400 : Loss 0.45343971252441406\n",
            "Step 500 : Loss 0.5390989780426025\n",
            "Step 600 : Loss 0.5275739431381226\n",
            "Step 700 : Loss 0.5447391867637634\n",
            "Step 800 : Loss 0.5630017518997192\n",
            "Step 900 : Loss 0.5559310913085938\n",
            "Step 1000 : Loss 0.4997626543045044\n",
            "Step 1100 : Loss 0.5389140248298645\n",
            "Step 1200 : Loss 0.5621635913848877\n",
            "Step 1300 : Loss 0.5521432161331177\n",
            "Step 1400 : Loss 0.4773652255535126\n",
            "Step 1500 : Loss 0.46930640935897827\n",
            "Step 1600 : Loss 0.5906533002853394\n",
            "Step 1700 : Loss 0.5662506222724915\n",
            "Step 1800 : Loss 0.5893092155456543\n",
            "Step 1900 : Loss 0.42824724316596985\n",
            "Step 2000 : Loss 0.5482033491134644\n",
            "Step 2100 : Loss 0.4760877192020416\n",
            "Step 2200 : Loss 0.5079092383384705\n",
            "Step 2300 : Loss 0.47947704792022705\n",
            "Step 2400 : Loss 0.5894140601158142\n",
            "Step 2500 : Loss 0.5216389894485474\n",
            "Step 2600 : Loss 0.4963420331478119\n",
            "Step 2700 : Loss 0.6170177459716797\n",
            "Step 2800 : Loss 0.6012569665908813\n",
            "Step 2900 : Loss 0.4580925405025482\n",
            "Step 3000 : Loss 0.5489795207977295\n",
            "Step 3100 : Loss 0.5364291071891785\n",
            "Step 3200 : Loss 0.5935573577880859\n",
            "Step 3300 : Loss 0.6112200021743774\n",
            "Step 3400 : Loss 0.5785940289497375\n",
            "Step 3500 : Loss 0.4959409832954407\n",
            "Step 3600 : Loss 0.5470595359802246\n",
            "Step 3700 : Loss 0.5802850723266602\n",
            "Step 3800 : Loss 0.5504874587059021\n",
            "Step 3900 : Loss 0.49226143956184387\n",
            "Step 4000 : Loss 0.5679327845573425\n",
            "Step 4100 : Loss 0.4883003234863281\n",
            "Step 4200 : Loss 0.49075353145599365\n",
            "Step 4300 : Loss 0.5666305422782898\n",
            "Step 4400 : Loss 0.5431339740753174\n",
            "Step 4500 : Loss 0.5058298707008362\n",
            "Step 4600 : Loss 0.450225293636322\n",
            "Step 4700 : Loss 0.47906264662742615\n",
            "Step 4800 : Loss 0.42319074273109436\n",
            "Step 4900 : Loss 0.5164492726325989\n",
            "Step 5000 : Loss 0.542010486125946\n",
            "Step 5100 : Loss 0.4745630621910095\n",
            "Step 5200 : Loss 0.5312774181365967\n",
            "Step 5300 : Loss 0.6009879112243652\n",
            "Step 5400 : Loss 0.5032646656036377\n",
            "Step 5500 : Loss 0.5248526334762573\n",
            "Step 5600 : Loss 0.4789397716522217\n",
            "Average loss at epoch 6 is 0.5228781299720422\n",
            "Average train accuracy at epoch 6 is 0.7557197835908305\n",
            "Step 100 : Loss 0.47608765959739685\n",
            "Step 200 : Loss 0.5707802772521973\n",
            "Step 300 : Loss 0.49664920568466187\n",
            "Step 400 : Loss 0.40726861357688904\n",
            "Step 500 : Loss 0.6023967862129211\n",
            "Step 600 : Loss 0.473183274269104\n",
            "Step 700 : Loss 0.45659342408180237\n",
            "Step 800 : Loss 0.545157790184021\n",
            "Step 900 : Loss 0.5191501975059509\n",
            "Step 1000 : Loss 0.588625431060791\n",
            "Step 1100 : Loss 0.4391975998878479\n",
            "Step 1200 : Loss 0.5030911564826965\n",
            "Step 1300 : Loss 0.5063967108726501\n",
            "Step 1400 : Loss 0.5506976246833801\n",
            "Step 1500 : Loss 0.4815170466899872\n",
            "Step 1600 : Loss 0.4731009006500244\n",
            "Step 1700 : Loss 0.4675489664077759\n",
            "Step 1800 : Loss 0.4736524224281311\n",
            "Step 1900 : Loss 0.45804598927497864\n",
            "Step 2000 : Loss 0.48578202724456787\n",
            "Step 2100 : Loss 0.4420313537120819\n",
            "Step 2200 : Loss 0.5536442995071411\n",
            "Step 2300 : Loss 0.45932772755622864\n",
            "Step 2400 : Loss 0.6225545406341553\n",
            "Step 2500 : Loss 0.4834139943122864\n",
            "Step 2600 : Loss 0.4100705683231354\n",
            "Step 2700 : Loss 0.5309566259384155\n",
            "Step 2800 : Loss 0.5076047778129578\n",
            "Step 2900 : Loss 0.5249139070510864\n",
            "Step 3000 : Loss 0.4460187554359436\n",
            "Step 3100 : Loss 0.4254680275917053\n",
            "Step 3200 : Loss 0.47819218039512634\n",
            "Step 3300 : Loss 0.5850557684898376\n",
            "Step 3400 : Loss 0.44639745354652405\n",
            "Step 3500 : Loss 0.5011250376701355\n",
            "Step 3600 : Loss 0.48838746547698975\n",
            "Step 3700 : Loss 0.5023863315582275\n",
            "Step 3800 : Loss 0.47996973991394043\n",
            "Step 3900 : Loss 0.5060802102088928\n",
            "Step 4000 : Loss 0.453499436378479\n",
            "Step 4100 : Loss 0.5102647542953491\n",
            "Step 4200 : Loss 0.41113561391830444\n",
            "Step 4300 : Loss 0.4799346327781677\n",
            "Step 4400 : Loss 0.5463788509368896\n",
            "Step 4500 : Loss 0.5696184039115906\n",
            "Step 4600 : Loss 0.49326133728027344\n",
            "Step 4700 : Loss 0.4439312517642975\n",
            "Step 4800 : Loss 0.5412195920944214\n",
            "Step 4900 : Loss 0.481579065322876\n",
            "Step 5000 : Loss 0.46665218472480774\n",
            "Step 5100 : Loss 0.474978506565094\n",
            "Step 5200 : Loss 0.5330407619476318\n",
            "Step 5300 : Loss 0.4938237965106964\n",
            "Step 5400 : Loss 0.4986420273780823\n",
            "Step 5500 : Loss 0.47147855162620544\n",
            "Step 5600 : Loss 0.4740961194038391\n",
            "Average loss at epoch 7 is 0.5113904275905654\n",
            "Average train accuracy at epoch 7 is 0.7639675944001338\n",
            "Step 100 : Loss 0.5697863101959229\n",
            "Step 200 : Loss 0.5032184720039368\n",
            "Step 300 : Loss 0.4480852782726288\n",
            "Step 400 : Loss 0.48806869983673096\n",
            "Step 500 : Loss 0.5017592906951904\n",
            "Step 600 : Loss 0.5151970386505127\n",
            "Step 700 : Loss 0.5297601222991943\n",
            "Step 800 : Loss 0.48735401034355164\n",
            "Step 900 : Loss 0.45277130603790283\n",
            "Step 1000 : Loss 0.5369251370429993\n",
            "Step 1100 : Loss 0.5636472702026367\n",
            "Step 1200 : Loss 0.535635232925415\n",
            "Step 1300 : Loss 0.5495277643203735\n",
            "Step 1400 : Loss 0.4924468398094177\n",
            "Step 1500 : Loss 0.519252598285675\n",
            "Step 1600 : Loss 0.4987758994102478\n",
            "Step 1700 : Loss 0.4758303761482239\n",
            "Step 1800 : Loss 0.5676360130310059\n",
            "Step 1900 : Loss 0.45079582929611206\n",
            "Step 2000 : Loss 0.5509201288223267\n",
            "Step 2100 : Loss 0.5661630630493164\n",
            "Step 2200 : Loss 0.5635325312614441\n",
            "Step 2300 : Loss 0.4957123398780823\n",
            "Step 2400 : Loss 0.48222774267196655\n",
            "Step 2500 : Loss 0.5064356923103333\n",
            "Step 2600 : Loss 0.4814198613166809\n",
            "Step 2700 : Loss 0.5217502117156982\n",
            "Step 2800 : Loss 0.5939033627510071\n",
            "Step 2900 : Loss 0.6311377882957458\n",
            "Step 3000 : Loss 0.5394513607025146\n",
            "Step 3100 : Loss 0.5590781569480896\n",
            "Step 3200 : Loss 0.45890557765960693\n",
            "Step 3300 : Loss 0.4097881317138672\n",
            "Step 3400 : Loss 0.4025239646434784\n",
            "Step 3500 : Loss 0.46775031089782715\n",
            "Step 3600 : Loss 0.42430004477500916\n",
            "Step 3700 : Loss 0.45212322473526\n",
            "Step 3800 : Loss 0.4602225720882416\n",
            "Step 3900 : Loss 0.4908302128314972\n",
            "Step 4000 : Loss 0.473041296005249\n",
            "Step 4100 : Loss 0.457162082195282\n",
            "Step 4200 : Loss 0.4505431056022644\n",
            "Step 4300 : Loss 0.4986710846424103\n",
            "Step 4400 : Loss 0.552383542060852\n",
            "Step 4500 : Loss 0.5263941287994385\n",
            "Step 4600 : Loss 0.42678332328796387\n",
            "Step 4700 : Loss 0.4843641221523285\n",
            "Step 4800 : Loss 0.4512649178504944\n",
            "Step 4900 : Loss 0.4631451666355133\n",
            "Step 5000 : Loss 0.5478515625\n",
            "Step 5100 : Loss 0.5146942138671875\n",
            "Step 5200 : Loss 0.49824827909469604\n",
            "Step 5300 : Loss 0.48209166526794434\n",
            "Step 5400 : Loss 0.541090190410614\n",
            "Step 5500 : Loss 0.5485556721687317\n",
            "Step 5600 : Loss 0.5014451742172241\n",
            "Average loss at epoch 8 is 0.5003980833431365\n",
            "Average train accuracy at epoch 8 is 0.7716409169501924\n",
            "Step 100 : Loss 0.4140670895576477\n",
            "Step 200 : Loss 0.42402347922325134\n",
            "Step 300 : Loss 0.5481135249137878\n",
            "Step 400 : Loss 0.3945387601852417\n",
            "Step 500 : Loss 0.490608811378479\n",
            "Step 600 : Loss 0.4789693355560303\n",
            "Step 700 : Loss 0.5227830410003662\n",
            "Step 800 : Loss 0.42194753885269165\n",
            "Step 900 : Loss 0.4769201874732971\n",
            "Step 1000 : Loss 0.42028307914733887\n",
            "Step 1100 : Loss 0.49037057161331177\n",
            "Step 1200 : Loss 0.5693591833114624\n",
            "Step 1300 : Loss 0.5783759951591492\n",
            "Step 1400 : Loss 0.5492309927940369\n",
            "Step 1500 : Loss 0.5079400539398193\n",
            "Step 1600 : Loss 0.5150195360183716\n",
            "Step 1700 : Loss 0.505154013633728\n",
            "Step 1800 : Loss 0.4672664403915405\n",
            "Step 1900 : Loss 0.4164797067642212\n",
            "Step 2000 : Loss 0.5319732427597046\n",
            "Step 2100 : Loss 0.5005966424942017\n",
            "Step 2200 : Loss 0.437575101852417\n",
            "Step 2300 : Loss 0.4851798117160797\n",
            "Step 2400 : Loss 0.5145684480667114\n",
            "Step 2500 : Loss 0.5034177899360657\n",
            "Step 2600 : Loss 0.48728835582733154\n",
            "Step 2700 : Loss 0.4853479862213135\n",
            "Step 2800 : Loss 0.45094066858291626\n",
            "Step 2900 : Loss 0.468315988779068\n",
            "Step 3000 : Loss 0.6012512445449829\n",
            "Step 3100 : Loss 0.5575315356254578\n",
            "Step 3200 : Loss 0.4945807158946991\n",
            "Step 3300 : Loss 0.5169490575790405\n",
            "Step 3400 : Loss 0.4112669825553894\n",
            "Step 3500 : Loss 0.441752165555954\n",
            "Step 3600 : Loss 0.5142216086387634\n",
            "Step 3700 : Loss 0.41170352697372437\n",
            "Step 3800 : Loss 0.43903428316116333\n",
            "Step 3900 : Loss 0.3796764314174652\n",
            "Step 4000 : Loss 0.425809383392334\n",
            "Step 4100 : Loss 0.4700157642364502\n",
            "Step 4200 : Loss 0.43527209758758545\n",
            "Step 4300 : Loss 0.4369630813598633\n",
            "Step 4400 : Loss 0.4222550392150879\n",
            "Step 4500 : Loss 0.5132281184196472\n",
            "Step 4600 : Loss 0.3794093132019043\n",
            "Step 4700 : Loss 0.5196782946586609\n",
            "Step 4800 : Loss 0.4198921322822571\n",
            "Step 4900 : Loss 0.4434579014778137\n",
            "Step 5000 : Loss 0.4399331212043762\n",
            "Step 5100 : Loss 0.4314371347427368\n",
            "Step 5200 : Loss 0.44875144958496094\n",
            "Step 5300 : Loss 0.4162565767765045\n",
            "Step 5400 : Loss 0.4483700394630432\n",
            "Step 5500 : Loss 0.45835191011428833\n",
            "Step 5600 : Loss 0.4494680166244507\n",
            "Average loss at epoch 9 is 0.4891503832286199\n",
            "Average train accuracy at epoch 9 is 0.7789210218082436\n",
            "Step 100 : Loss 0.42999646067619324\n",
            "Step 200 : Loss 0.5471700429916382\n",
            "Step 300 : Loss 0.40366053581237793\n",
            "Step 400 : Loss 0.45772308111190796\n",
            "Step 500 : Loss 0.48604080080986023\n",
            "Step 600 : Loss 0.4388890564441681\n",
            "Step 700 : Loss 0.5530095100402832\n",
            "Step 800 : Loss 0.5116730332374573\n",
            "Step 900 : Loss 0.4958491921424866\n",
            "Step 1000 : Loss 0.462047278881073\n",
            "Step 1100 : Loss 0.5500974655151367\n",
            "Step 1200 : Loss 0.39785462617874146\n",
            "Step 1300 : Loss 0.5514453649520874\n",
            "Step 1400 : Loss 0.41789084672927856\n",
            "Step 1500 : Loss 0.4531075954437256\n",
            "Step 1600 : Loss 0.4789351224899292\n",
            "Step 1700 : Loss 0.5072751045227051\n",
            "Step 1800 : Loss 0.501667857170105\n",
            "Step 1900 : Loss 0.4801054894924164\n",
            "Step 2000 : Loss 0.49219149351119995\n",
            "Step 2100 : Loss 0.5145832896232605\n",
            "Step 2200 : Loss 0.5127936601638794\n",
            "Step 2300 : Loss 0.4784255623817444\n",
            "Step 2400 : Loss 0.46050411462783813\n",
            "Step 2500 : Loss 0.43593764305114746\n",
            "Step 2600 : Loss 0.45712554454803467\n",
            "Step 2700 : Loss 0.48238539695739746\n",
            "Step 2800 : Loss 0.45151787996292114\n",
            "Step 2900 : Loss 0.4616202712059021\n",
            "Step 3000 : Loss 0.5538277626037598\n",
            "Step 3100 : Loss 0.4440896809101105\n",
            "Step 3200 : Loss 0.4960542321205139\n",
            "Step 3300 : Loss 0.511562168598175\n",
            "Step 3400 : Loss 0.4194331765174866\n",
            "Step 3500 : Loss 0.4839760661125183\n",
            "Step 3600 : Loss 0.39600059390068054\n",
            "Step 3700 : Loss 0.4425973892211914\n",
            "Step 3800 : Loss 0.5188456177711487\n",
            "Step 3900 : Loss 0.46242693066596985\n",
            "Step 4000 : Loss 0.5365736484527588\n",
            "Step 4100 : Loss 0.4568045139312744\n",
            "Step 4200 : Loss 0.5015568733215332\n",
            "Step 4300 : Loss 0.4470314681529999\n",
            "Step 4400 : Loss 0.4047195017337799\n",
            "Step 4500 : Loss 0.3978434205055237\n",
            "Step 4600 : Loss 0.4742061495780945\n",
            "Step 4700 : Loss 0.5726379752159119\n",
            "Step 4800 : Loss 0.42100226879119873\n",
            "Step 4900 : Loss 0.5296903848648071\n",
            "Step 5000 : Loss 0.43788331747055054\n",
            "Step 5100 : Loss 0.45007091760635376\n",
            "Step 5200 : Loss 0.5038303732872009\n",
            "Step 5300 : Loss 0.5018723011016846\n",
            "Step 5400 : Loss 0.5198936462402344\n",
            "Step 5500 : Loss 0.42196354269981384\n",
            "Step 5600 : Loss 0.4761005640029907\n",
            "Average loss at epoch 10 is 0.47745905220561763\n",
            "Average train accuracy at epoch 10 is 0.7844553516649004\n",
            "Step 100 : Loss 0.5217397809028625\n",
            "Step 200 : Loss 0.46067574620246887\n",
            "Step 300 : Loss 0.4316229224205017\n",
            "Step 400 : Loss 0.483061283826828\n",
            "Step 500 : Loss 0.4962310194969177\n",
            "Step 600 : Loss 0.4466268718242645\n",
            "Step 700 : Loss 0.4819757640361786\n",
            "Step 800 : Loss 0.4997302293777466\n",
            "Step 900 : Loss 0.44089776277542114\n",
            "Step 1000 : Loss 0.5301028490066528\n",
            "Step 1100 : Loss 0.4772096872329712\n",
            "Step 1200 : Loss 0.4937061071395874\n",
            "Step 1300 : Loss 0.40918073058128357\n",
            "Step 1400 : Loss 0.43670594692230225\n",
            "Step 1500 : Loss 0.38503700494766235\n",
            "Step 1600 : Loss 0.46202829480171204\n",
            "Step 1700 : Loss 0.48261475563049316\n",
            "Step 1800 : Loss 0.4159665107727051\n",
            "Step 1900 : Loss 0.4707890450954437\n",
            "Step 2000 : Loss 0.46125906705856323\n",
            "Step 2100 : Loss 0.4793248772621155\n",
            "Step 2200 : Loss 0.4701365530490875\n",
            "Step 2300 : Loss 0.4637698829174042\n",
            "Step 2400 : Loss 0.42379263043403625\n",
            "Step 2500 : Loss 0.5015943646430969\n",
            "Step 2600 : Loss 0.43672043085098267\n",
            "Step 2700 : Loss 0.47185584902763367\n",
            "Step 2800 : Loss 0.40080201625823975\n",
            "Step 2900 : Loss 0.4991079270839691\n",
            "Step 3000 : Loss 0.4775976538658142\n",
            "Step 3100 : Loss 0.48645880818367004\n",
            "Step 3200 : Loss 0.4118349850177765\n",
            "Step 3300 : Loss 0.39920687675476074\n",
            "Step 3400 : Loss 0.4940597414970398\n",
            "Step 3500 : Loss 0.472790002822876\n",
            "Step 3600 : Loss 0.41182246804237366\n",
            "Step 3700 : Loss 0.4728027284145355\n",
            "Step 3800 : Loss 0.42696335911750793\n",
            "Step 3900 : Loss 0.49742770195007324\n",
            "Step 4000 : Loss 0.41520214080810547\n",
            "Step 4100 : Loss 0.433233380317688\n",
            "Step 4200 : Loss 0.5266849994659424\n",
            "Step 4300 : Loss 0.48680931329727173\n",
            "Step 4400 : Loss 0.37829914689064026\n",
            "Step 4500 : Loss 0.4378840923309326\n",
            "Step 4600 : Loss 0.4722256660461426\n",
            "Step 4700 : Loss 0.4086877405643463\n",
            "Step 4800 : Loss 0.5334432125091553\n",
            "Step 4900 : Loss 0.4595510959625244\n",
            "Step 5000 : Loss 0.43528348207473755\n",
            "Step 5100 : Loss 0.44240644574165344\n",
            "Step 5200 : Loss 0.4146187901496887\n",
            "Step 5300 : Loss 0.4323210120201111\n",
            "Step 5400 : Loss 0.4971970319747925\n",
            "Step 5500 : Loss 0.47980016469955444\n",
            "Step 5600 : Loss 0.47347649931907654\n",
            "Average loss at epoch 11 is 0.4648798920311248\n",
            "Average train accuracy at epoch 11 is 0.790108204584751\n",
            "Step 100 : Loss 0.533004879951477\n",
            "Step 200 : Loss 0.42843806743621826\n",
            "Step 300 : Loss 0.48904070258140564\n",
            "Step 400 : Loss 0.4551241099834442\n",
            "Step 500 : Loss 0.3776213228702545\n",
            "Step 600 : Loss 0.40677589178085327\n",
            "Step 700 : Loss 0.45793071389198303\n",
            "Step 800 : Loss 0.4330978989601135\n",
            "Step 900 : Loss 0.4553423821926117\n",
            "Step 1000 : Loss 0.4492747187614441\n",
            "Step 1100 : Loss 0.46966978907585144\n",
            "Step 1200 : Loss 0.4141647219657898\n",
            "Step 1300 : Loss 0.4692381024360657\n",
            "Step 1400 : Loss 0.39027416706085205\n",
            "Step 1500 : Loss 0.47838789224624634\n",
            "Step 1600 : Loss 0.383084237575531\n",
            "Step 1700 : Loss 0.5012811422348022\n",
            "Step 1800 : Loss 0.3635941445827484\n",
            "Step 1900 : Loss 0.4361463785171509\n",
            "Step 2000 : Loss 0.3875015676021576\n",
            "Step 2100 : Loss 0.3753770589828491\n",
            "Step 2200 : Loss 0.4469493329524994\n",
            "Step 2300 : Loss 0.37836700677871704\n",
            "Step 2400 : Loss 0.47981932759284973\n",
            "Step 2500 : Loss 0.4911964237689972\n",
            "Step 2600 : Loss 0.4941820502281189\n",
            "Step 2700 : Loss 0.5197744369506836\n",
            "Step 2800 : Loss 0.3976094722747803\n",
            "Step 2900 : Loss 0.46394896507263184\n",
            "Step 3000 : Loss 0.48742759227752686\n",
            "Step 3100 : Loss 0.4618079364299774\n",
            "Step 3200 : Loss 0.5567660927772522\n",
            "Step 3300 : Loss 0.39377275109291077\n",
            "Step 3400 : Loss 0.4313497245311737\n",
            "Step 3500 : Loss 0.38873690366744995\n",
            "Step 3600 : Loss 0.443154513835907\n",
            "Step 3700 : Loss 0.47604846954345703\n",
            "Step 3800 : Loss 0.4909507632255554\n",
            "Step 3900 : Loss 0.4485386908054352\n",
            "Step 4000 : Loss 0.3898553252220154\n",
            "Step 4100 : Loss 0.39707446098327637\n",
            "Step 4200 : Loss 0.49862369894981384\n",
            "Step 4300 : Loss 0.43754035234451294\n",
            "Step 4400 : Loss 0.4190278947353363\n",
            "Step 4500 : Loss 0.3725999593734741\n",
            "Step 4600 : Loss 0.4484108090400696\n",
            "Step 4700 : Loss 0.505541205406189\n",
            "Step 4800 : Loss 0.3920786678791046\n",
            "Step 4900 : Loss 0.3762449622154236\n",
            "Step 5000 : Loss 0.4882391691207886\n",
            "Step 5100 : Loss 0.4617209732532501\n",
            "Step 5200 : Loss 0.3545573055744171\n",
            "Step 5300 : Loss 0.41533538699150085\n",
            "Step 5400 : Loss 0.48830410838127136\n",
            "Step 5500 : Loss 0.3779643476009369\n",
            "Step 5600 : Loss 0.4725276231765747\n",
            "Average loss at epoch 12 is 0.45109262275797923\n",
            "Average train accuracy at epoch 12 is 0.7955560823247253\n",
            "Step 100 : Loss 0.4239979386329651\n",
            "Step 200 : Loss 0.4946998059749603\n",
            "Step 300 : Loss 0.43299049139022827\n",
            "Step 400 : Loss 0.4800907075405121\n",
            "Step 500 : Loss 0.40794217586517334\n",
            "Step 600 : Loss 0.37546858191490173\n",
            "Step 700 : Loss 0.463399738073349\n",
            "Step 800 : Loss 0.3961372375488281\n",
            "Step 900 : Loss 0.4451359510421753\n",
            "Step 1000 : Loss 0.3677310347557068\n",
            "Step 1100 : Loss 0.3577425479888916\n",
            "Step 1200 : Loss 0.45095473527908325\n",
            "Step 1300 : Loss 0.4320914149284363\n",
            "Step 1400 : Loss 0.4667690396308899\n",
            "Step 1500 : Loss 0.4124048948287964\n",
            "Step 1600 : Loss 0.4305298924446106\n",
            "Step 1700 : Loss 0.5412322282791138\n",
            "Step 1800 : Loss 0.475976824760437\n",
            "Step 1900 : Loss 0.38699424266815186\n",
            "Step 2000 : Loss 0.4601559042930603\n",
            "Step 2100 : Loss 0.4307363033294678\n",
            "Step 2200 : Loss 0.46257179975509644\n",
            "Step 2300 : Loss 0.42735832929611206\n",
            "Step 2400 : Loss 0.4556727707386017\n",
            "Step 2500 : Loss 0.5248125791549683\n",
            "Step 2600 : Loss 0.42900148034095764\n",
            "Step 2700 : Loss 0.4152452051639557\n",
            "Step 2800 : Loss 0.38132143020629883\n",
            "Step 2900 : Loss 0.457991361618042\n",
            "Step 3000 : Loss 0.4699685275554657\n",
            "Step 3100 : Loss 0.4488404393196106\n",
            "Step 3200 : Loss 0.4110337197780609\n",
            "Step 3300 : Loss 0.47753265500068665\n",
            "Step 3400 : Loss 0.4470558166503906\n",
            "Step 3500 : Loss 0.5113544464111328\n",
            "Step 3600 : Loss 0.4452918469905853\n",
            "Step 3700 : Loss 0.4358319342136383\n",
            "Step 3800 : Loss 0.44602838158607483\n",
            "Step 3900 : Loss 0.4406617283821106\n",
            "Step 4000 : Loss 0.39850136637687683\n",
            "Step 4100 : Loss 0.4280075430870056\n",
            "Step 4200 : Loss 0.4695703983306885\n",
            "Step 4300 : Loss 0.40722566843032837\n",
            "Step 4400 : Loss 0.4383878707885742\n",
            "Step 4500 : Loss 0.3857234716415405\n",
            "Step 4600 : Loss 0.5310192704200745\n",
            "Step 4700 : Loss 0.4226439893245697\n",
            "Step 4800 : Loss 0.3776407241821289\n",
            "Step 4900 : Loss 0.4809317886829376\n",
            "Step 5000 : Loss 0.4580442011356354\n",
            "Step 5100 : Loss 0.415549635887146\n",
            "Step 5200 : Loss 0.4766528308391571\n",
            "Step 5300 : Loss 0.4848228991031647\n",
            "Step 5400 : Loss 0.5129849910736084\n",
            "Step 5500 : Loss 0.4668293595314026\n",
            "Step 5600 : Loss 0.42250874638557434\n",
            "Average loss at epoch 13 is 0.43598741054662565\n",
            "Average train accuracy at epoch 13 is 0.8024025322103854\n",
            "Step 100 : Loss 0.3803859353065491\n",
            "Step 200 : Loss 0.430562287569046\n",
            "Step 300 : Loss 0.39132601022720337\n",
            "Step 400 : Loss 0.513811469078064\n",
            "Step 500 : Loss 0.5786405801773071\n",
            "Step 600 : Loss 0.3375885784626007\n",
            "Step 700 : Loss 0.4806640148162842\n",
            "Step 800 : Loss 0.4051415026187897\n",
            "Step 900 : Loss 0.4200509786605835\n",
            "Step 1000 : Loss 0.4305654764175415\n",
            "Step 1100 : Loss 0.46056604385375977\n",
            "Step 1200 : Loss 0.39821216464042664\n",
            "Step 1300 : Loss 0.4604065418243408\n",
            "Step 1400 : Loss 0.40908318758010864\n",
            "Step 1500 : Loss 0.38385847210884094\n",
            "Step 1600 : Loss 0.38889288902282715\n",
            "Step 1700 : Loss 0.47631001472473145\n",
            "Step 1800 : Loss 0.3866913914680481\n",
            "Step 1900 : Loss 0.4060748815536499\n",
            "Step 2000 : Loss 0.42961782217025757\n",
            "Step 2100 : Loss 0.4589345455169678\n",
            "Step 2200 : Loss 0.36824655532836914\n",
            "Step 2300 : Loss 0.336133748292923\n",
            "Step 2400 : Loss 0.41680610179901123\n",
            "Step 2500 : Loss 0.44483351707458496\n",
            "Step 2600 : Loss 0.39413830637931824\n",
            "Step 2700 : Loss 0.4366293251514435\n",
            "Step 2800 : Loss 0.34438538551330566\n",
            "Step 2900 : Loss 0.39559000730514526\n",
            "Step 3000 : Loss 0.5296550393104553\n",
            "Step 3100 : Loss 0.4508349597454071\n",
            "Step 3200 : Loss 0.45914947986602783\n",
            "Step 3300 : Loss 0.37821638584136963\n",
            "Step 3400 : Loss 0.3620888590812683\n",
            "Step 3500 : Loss 0.48203539848327637\n",
            "Step 3600 : Loss 0.39518749713897705\n",
            "Step 3700 : Loss 0.4366498291492462\n",
            "Step 3800 : Loss 0.5197390913963318\n",
            "Step 3900 : Loss 0.3833630681037903\n",
            "Step 4000 : Loss 0.37413084506988525\n",
            "Step 4100 : Loss 0.4512467086315155\n",
            "Step 4200 : Loss 0.3966387212276459\n",
            "Step 4300 : Loss 0.37789642810821533\n",
            "Step 4400 : Loss 0.41099393367767334\n",
            "Step 4500 : Loss 0.37352967262268066\n",
            "Step 4600 : Loss 0.3740830719470978\n",
            "Step 4700 : Loss 0.4134569764137268\n",
            "Step 4800 : Loss 0.4062420129776001\n",
            "Step 4900 : Loss 0.39278462529182434\n",
            "Step 5000 : Loss 0.5287526845932007\n",
            "Step 5100 : Loss 0.4123537242412567\n",
            "Step 5200 : Loss 0.4109751582145691\n",
            "Step 5300 : Loss 0.39621132612228394\n",
            "Step 5400 : Loss 0.4004320204257965\n",
            "Step 5500 : Loss 0.3979441523551941\n",
            "Step 5600 : Loss 0.5092547535896301\n",
            "Average loss at epoch 14 is 0.42133186094020064\n",
            "Average train accuracy at epoch 14 is 0.8084095599308383\n",
            "Step 100 : Loss 0.45625025033950806\n",
            "Step 200 : Loss 0.39981621503829956\n",
            "Step 300 : Loss 0.47634702920913696\n",
            "Step 400 : Loss 0.41632431745529175\n",
            "Step 500 : Loss 0.42698371410369873\n",
            "Step 600 : Loss 0.42035797238349915\n",
            "Step 700 : Loss 0.3904561996459961\n",
            "Step 800 : Loss 0.355793833732605\n",
            "Step 900 : Loss 0.35412126779556274\n",
            "Step 1000 : Loss 0.4414074718952179\n",
            "Step 1100 : Loss 0.3685799837112427\n",
            "Step 1200 : Loss 0.3805081844329834\n",
            "Step 1300 : Loss 0.38706398010253906\n",
            "Step 1400 : Loss 0.3868478536605835\n",
            "Step 1500 : Loss 0.3864719867706299\n",
            "Step 1600 : Loss 0.47525015473365784\n",
            "Step 1700 : Loss 0.42606621980667114\n",
            "Step 1800 : Loss 0.3829871416091919\n",
            "Step 1900 : Loss 0.4045338034629822\n",
            "Step 2000 : Loss 0.3597007691860199\n",
            "Step 2100 : Loss 0.3932090401649475\n",
            "Step 2200 : Loss 0.43530869483947754\n",
            "Step 2300 : Loss 0.3550405502319336\n",
            "Step 2400 : Loss 0.3685021698474884\n",
            "Step 2500 : Loss 0.4300481975078583\n",
            "Step 2600 : Loss 0.465920090675354\n",
            "Step 2700 : Loss 0.4158843457698822\n",
            "Step 2800 : Loss 0.5036557912826538\n",
            "Step 2900 : Loss 0.35584756731987\n",
            "Step 3000 : Loss 0.35624033212661743\n",
            "Step 3100 : Loss 0.3782039284706116\n",
            "Step 3200 : Loss 0.4209253191947937\n",
            "Step 3300 : Loss 0.44473177194595337\n",
            "Step 3400 : Loss 0.40029552578926086\n",
            "Step 3500 : Loss 0.47372084856033325\n",
            "Step 3600 : Loss 0.4656851291656494\n",
            "Step 3700 : Loss 0.4602314829826355\n",
            "Step 3800 : Loss 0.44506973028182983\n",
            "Step 3900 : Loss 0.4006737172603607\n",
            "Step 4000 : Loss 0.39444243907928467\n",
            "Step 4100 : Loss 0.49545708298683167\n",
            "Step 4200 : Loss 0.4681050181388855\n",
            "Step 4300 : Loss 0.3794841468334198\n",
            "Step 4400 : Loss 0.33805644512176514\n",
            "Step 4500 : Loss 0.36923253536224365\n",
            "Step 4600 : Loss 0.39142918586730957\n",
            "Step 4700 : Loss 0.5047116279602051\n",
            "Step 4800 : Loss 0.4496999979019165\n",
            "Step 4900 : Loss 0.40564319491386414\n",
            "Step 5000 : Loss 0.4635397791862488\n",
            "Step 5100 : Loss 0.41894054412841797\n",
            "Step 5200 : Loss 0.40352821350097656\n",
            "Step 5300 : Loss 0.5072276592254639\n",
            "Step 5400 : Loss 0.3778730630874634\n",
            "Step 5500 : Loss 0.3585411310195923\n",
            "Step 5600 : Loss 0.30995237827301025\n",
            "Average loss at epoch 15 is 0.40725576035065375\n",
            "Average train accuracy at epoch 15 is 0.8148586089575548\n",
            "Step 100 : Loss 0.4091084599494934\n",
            "Step 200 : Loss 0.4506142735481262\n",
            "Step 300 : Loss 0.4130030870437622\n",
            "Step 400 : Loss 0.4257235527038574\n",
            "Step 500 : Loss 0.4820265471935272\n",
            "Step 600 : Loss 0.40359407663345337\n",
            "Step 700 : Loss 0.340273916721344\n",
            "Step 800 : Loss 0.3833546042442322\n",
            "Step 900 : Loss 0.3304460048675537\n",
            "Step 1000 : Loss 0.3797358274459839\n",
            "Step 1100 : Loss 0.3409891128540039\n",
            "Step 1200 : Loss 0.32331907749176025\n",
            "Step 1300 : Loss 0.43603575229644775\n",
            "Step 1400 : Loss 0.4786399006843567\n",
            "Step 1500 : Loss 0.34392356872558594\n",
            "Step 1600 : Loss 0.42744094133377075\n",
            "Step 1700 : Loss 0.3033057749271393\n",
            "Step 1800 : Loss 0.3670642375946045\n",
            "Step 1900 : Loss 0.33360904455184937\n",
            "Step 2000 : Loss 0.3341042399406433\n",
            "Step 2100 : Loss 0.4658375382423401\n",
            "Step 2200 : Loss 0.4672233760356903\n",
            "Step 2300 : Loss 0.47034722566604614\n",
            "Step 2400 : Loss 0.41495561599731445\n",
            "Step 2500 : Loss 0.351709246635437\n",
            "Step 2600 : Loss 0.40281155705451965\n",
            "Step 2700 : Loss 0.3899487257003784\n",
            "Step 2800 : Loss 0.45663514733314514\n",
            "Step 2900 : Loss 0.3818683326244354\n",
            "Step 3000 : Loss 0.42515987157821655\n",
            "Step 3100 : Loss 0.5130719542503357\n",
            "Step 3200 : Loss 0.43492037057876587\n",
            "Step 3300 : Loss 0.34144747257232666\n",
            "Step 3400 : Loss 0.3194938600063324\n",
            "Step 3500 : Loss 0.38204067945480347\n",
            "Step 3600 : Loss 0.46676886081695557\n",
            "Step 3700 : Loss 0.34968602657318115\n",
            "Step 3800 : Loss 0.4001939296722412\n",
            "Step 3900 : Loss 0.43442511558532715\n",
            "Step 4000 : Loss 0.3921509385108948\n",
            "Step 4100 : Loss 0.46455585956573486\n",
            "Step 4200 : Loss 0.37798023223876953\n",
            "Step 4300 : Loss 0.278214693069458\n",
            "Step 4400 : Loss 0.410593181848526\n",
            "Step 4500 : Loss 0.4110093414783478\n",
            "Step 4600 : Loss 0.491177499294281\n",
            "Step 4700 : Loss 0.4320563077926636\n",
            "Step 4800 : Loss 0.4471089243888855\n",
            "Step 4900 : Loss 0.3704672157764435\n",
            "Step 5000 : Loss 0.4329156279563904\n",
            "Step 5100 : Loss 0.4098145365715027\n",
            "Step 5200 : Loss 0.35236820578575134\n",
            "Step 5300 : Loss 0.45361119508743286\n",
            "Step 5400 : Loss 0.43406641483306885\n",
            "Step 5500 : Loss 0.3399915099143982\n",
            "Step 5600 : Loss 0.32455456256866455\n",
            "Average loss at epoch 16 is 0.39456341347161644\n",
            "Average train accuracy at epoch 16 is 0.8200317920687155\n",
            "Step 100 : Loss 0.4113095998764038\n",
            "Step 200 : Loss 0.35242417454719543\n",
            "Step 300 : Loss 0.36885881423950195\n",
            "Step 400 : Loss 0.39756011962890625\n",
            "Step 500 : Loss 0.48250800371170044\n",
            "Step 600 : Loss 0.36716020107269287\n",
            "Step 700 : Loss 0.30406004190444946\n",
            "Step 800 : Loss 0.43842899799346924\n",
            "Step 900 : Loss 0.39174753427505493\n",
            "Step 1000 : Loss 0.41364729404449463\n",
            "Step 1100 : Loss 0.35903793573379517\n",
            "Step 1200 : Loss 0.46065834164619446\n",
            "Step 1300 : Loss 0.3744391202926636\n",
            "Step 1400 : Loss 0.45504647493362427\n",
            "Step 1500 : Loss 0.30559781193733215\n",
            "Step 1600 : Loss 0.4134087562561035\n",
            "Step 1700 : Loss 0.33897167444229126\n",
            "Step 1800 : Loss 0.4197864532470703\n",
            "Step 1900 : Loss 0.3547114133834839\n",
            "Step 2000 : Loss 0.41195863485336304\n",
            "Step 2100 : Loss 0.34054508805274963\n",
            "Step 2200 : Loss 0.31921520829200745\n",
            "Step 2300 : Loss 0.41994738578796387\n",
            "Step 2400 : Loss 0.3747765123844147\n",
            "Step 2500 : Loss 0.3747175335884094\n",
            "Step 2600 : Loss 0.345143586397171\n",
            "Step 2700 : Loss 0.38654935359954834\n",
            "Step 2800 : Loss 0.3686645030975342\n",
            "Step 2900 : Loss 0.37494367361068726\n",
            "Step 3000 : Loss 0.43805497884750366\n",
            "Step 3100 : Loss 0.38078540563583374\n",
            "Step 3200 : Loss 0.3701757788658142\n",
            "Step 3300 : Loss 0.3355824947357178\n",
            "Step 3400 : Loss 0.3258473873138428\n",
            "Step 3500 : Loss 0.47881340980529785\n",
            "Step 3600 : Loss 0.32975319027900696\n",
            "Step 3700 : Loss 0.3620187044143677\n",
            "Step 3800 : Loss 0.45313185453414917\n",
            "Step 3900 : Loss 0.4375646114349365\n",
            "Step 4000 : Loss 0.36504802107810974\n",
            "Step 4100 : Loss 0.3921937346458435\n",
            "Step 4200 : Loss 0.3499670922756195\n",
            "Step 4300 : Loss 0.45703303813934326\n",
            "Step 4400 : Loss 0.37751084566116333\n",
            "Step 4500 : Loss 0.34686264395713806\n",
            "Step 4600 : Loss 0.318479061126709\n",
            "Step 4700 : Loss 0.40935853123664856\n",
            "Step 4800 : Loss 0.3025943636894226\n",
            "Step 4900 : Loss 0.4039708077907562\n",
            "Step 5000 : Loss 0.3293409049510956\n",
            "Step 5100 : Loss 0.4475943446159363\n",
            "Step 5200 : Loss 0.3710247874259949\n",
            "Step 5300 : Loss 0.37285739183425903\n",
            "Step 5400 : Loss 0.3935314416885376\n",
            "Step 5500 : Loss 0.38971981406211853\n",
            "Step 5600 : Loss 0.3809015452861786\n",
            "Average loss at epoch 17 is 0.38301061092211525\n",
            "Average train accuracy at epoch 17 is 0.8250808745607675\n",
            "Step 100 : Loss 0.4025118350982666\n",
            "Step 200 : Loss 0.4773704409599304\n",
            "Step 300 : Loss 0.38349461555480957\n",
            "Step 400 : Loss 0.39643868803977966\n",
            "Step 500 : Loss 0.38970306515693665\n",
            "Step 600 : Loss 0.338792622089386\n",
            "Step 700 : Loss 0.45099276304244995\n",
            "Step 800 : Loss 0.3227989971637726\n",
            "Step 900 : Loss 0.4307557940483093\n",
            "Step 1000 : Loss 0.3650681972503662\n",
            "Step 1100 : Loss 0.40972837805747986\n",
            "Step 1200 : Loss 0.41591504216194153\n",
            "Step 1300 : Loss 0.3471750319004059\n",
            "Step 1400 : Loss 0.4476492702960968\n",
            "Step 1500 : Loss 0.36014705896377563\n",
            "Step 1600 : Loss 0.4428063929080963\n",
            "Step 1700 : Loss 0.4443018436431885\n",
            "Step 1800 : Loss 0.36243337392807007\n",
            "Step 1900 : Loss 0.36604225635528564\n",
            "Step 2000 : Loss 0.41134631633758545\n",
            "Step 2100 : Loss 0.36389029026031494\n",
            "Step 2200 : Loss 0.41212141513824463\n",
            "Step 2300 : Loss 0.3758692145347595\n",
            "Step 2400 : Loss 0.4085122346878052\n",
            "Step 2500 : Loss 0.3347555994987488\n",
            "Step 2600 : Loss 0.3798154890537262\n",
            "Step 2700 : Loss 0.31048309803009033\n",
            "Step 2800 : Loss 0.4006364941596985\n",
            "Step 2900 : Loss 0.32463574409484863\n",
            "Step 3000 : Loss 0.4016113877296448\n",
            "Step 3100 : Loss 0.3581382632255554\n",
            "Step 3200 : Loss 0.4124683737754822\n",
            "Step 3300 : Loss 0.37725311517715454\n",
            "Step 3400 : Loss 0.2910102605819702\n",
            "Step 3500 : Loss 0.4319935441017151\n",
            "Step 3600 : Loss 0.3589671850204468\n",
            "Step 3700 : Loss 0.3341691493988037\n",
            "Step 3800 : Loss 0.43275541067123413\n",
            "Step 3900 : Loss 0.4212908148765564\n",
            "Step 4000 : Loss 0.34720322489738464\n",
            "Step 4100 : Loss 0.35607004165649414\n",
            "Step 4200 : Loss 0.3531484603881836\n",
            "Step 4300 : Loss 0.3816394507884979\n",
            "Step 4400 : Loss 0.28687024116516113\n",
            "Step 4500 : Loss 0.3045777678489685\n",
            "Step 4600 : Loss 0.3661479949951172\n",
            "Step 4700 : Loss 0.3966537415981293\n",
            "Step 4800 : Loss 0.33221369981765747\n",
            "Step 4900 : Loss 0.3531893193721771\n",
            "Step 5000 : Loss 0.33868923783302307\n",
            "Step 5100 : Loss 0.3324819803237915\n",
            "Step 5200 : Loss 0.3390190601348877\n",
            "Step 5300 : Loss 0.3432013988494873\n",
            "Step 5400 : Loss 0.33177489042282104\n",
            "Step 5500 : Loss 0.3605738878250122\n",
            "Step 5600 : Loss 0.39143767952919006\n",
            "Average loss at epoch 18 is 0.373109977510855\n",
            "Average train accuracy at epoch 18 is 0.8285152546154275\n",
            "Step 100 : Loss 0.4124526381492615\n",
            "Step 200 : Loss 0.35376399755477905\n",
            "Step 300 : Loss 0.46157753467559814\n",
            "Step 400 : Loss 0.35550302267074585\n",
            "Step 500 : Loss 0.35450512170791626\n",
            "Step 600 : Loss 0.3583737015724182\n",
            "Step 700 : Loss 0.36260804533958435\n",
            "Step 800 : Loss 0.40824949741363525\n",
            "Step 900 : Loss 0.2770015001296997\n",
            "Step 1000 : Loss 0.2629645764827728\n",
            "Step 1100 : Loss 0.35639700293540955\n",
            "Step 1200 : Loss 0.3926953375339508\n",
            "Step 1300 : Loss 0.36157068610191345\n",
            "Step 1400 : Loss 0.41214415431022644\n",
            "Step 1500 : Loss 0.3672022223472595\n",
            "Step 1600 : Loss 0.3422621488571167\n",
            "Step 1700 : Loss 0.2756543755531311\n",
            "Step 1800 : Loss 0.28952711820602417\n",
            "Step 1900 : Loss 0.4015938639640808\n",
            "Step 2000 : Loss 0.32098114490509033\n",
            "Step 2100 : Loss 0.4019505977630615\n",
            "Step 2200 : Loss 0.3581041395664215\n",
            "Step 2300 : Loss 0.3511600196361542\n",
            "Step 2400 : Loss 0.29016318917274475\n",
            "Step 2500 : Loss 0.3395434617996216\n",
            "Step 2600 : Loss 0.3961769640445709\n",
            "Step 2700 : Loss 0.5009535551071167\n",
            "Step 2800 : Loss 0.28788650035858154\n",
            "Step 2900 : Loss 0.3724396824836731\n",
            "Step 3000 : Loss 0.36702072620391846\n",
            "Step 3100 : Loss 0.3863917589187622\n",
            "Step 3200 : Loss 0.4394482374191284\n",
            "Step 3300 : Loss 0.3719959259033203\n",
            "Step 3400 : Loss 0.35336244106292725\n",
            "Step 3500 : Loss 0.382621169090271\n",
            "Step 3600 : Loss 0.3120113015174866\n",
            "Step 3700 : Loss 0.35485056042671204\n",
            "Step 3800 : Loss 0.3779337406158447\n",
            "Step 3900 : Loss 0.3820268511772156\n",
            "Step 4000 : Loss 0.289555162191391\n",
            "Step 4100 : Loss 0.34528475999832153\n",
            "Step 4200 : Loss 0.31268149614334106\n",
            "Step 4300 : Loss 0.2529239058494568\n",
            "Step 4400 : Loss 0.4237452447414398\n",
            "Step 4500 : Loss 0.4097132682800293\n",
            "Step 4600 : Loss 0.4116062521934509\n",
            "Step 4700 : Loss 0.3107655644416809\n",
            "Step 4800 : Loss 0.3464685082435608\n",
            "Step 4900 : Loss 0.3339553475379944\n",
            "Step 5000 : Loss 0.34246236085891724\n",
            "Step 5100 : Loss 0.3318345546722412\n",
            "Step 5200 : Loss 0.3456515073776245\n",
            "Step 5300 : Loss 0.32755371928215027\n",
            "Step 5400 : Loss 0.3375498950481415\n",
            "Step 5500 : Loss 0.434486985206604\n",
            "Step 5600 : Loss 0.3608538806438446\n",
            "Average loss at epoch 19 is 0.3638463447572886\n",
            "Average train accuracy at epoch 19 is 0.83212532768141\n",
            "Average Validation accuracy 0.8113020833333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}